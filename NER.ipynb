{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9a0tNAnTTNIfX6mkon2lT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaa-alt/NLP/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hpR4oAo4pEyh"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('eriktks/conll2003')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbeoH-nspNyU",
        "outputId": "8c12526e-ce77-4a8d-8b58-d1e3611b8420"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "eJh8RmvqqaW7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = dataset['train'].features['ner_tags'].feature.names"
      ],
      "metadata": {
        "id": "AvkqTelhts3V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91j3EySEurhh",
        "outputId": "bcffed3d-dda1-4a4e-b1dd-498fb6121a82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import DocBin\n",
        "def convert_to_spacy(data, output_path, label_list):\n",
        "    db = DocBin()\n",
        "    for example in data:\n",
        "        tokens = example[\"tokens\"]\n",
        "        ner_tags = example[\"ner_tags\"]\n",
        "\n",
        "        text = \" \".join(tokens)\n",
        "        doc = nlp.make_doc(text)\n",
        "\n",
        "        # Compute token start and end character offsets\n",
        "        token_offsets = []\n",
        "        current_pos = 0\n",
        "        for token in tokens:\n",
        "            start = current_pos\n",
        "            end = start + len(token)\n",
        "            token_offsets.append((start, end))\n",
        "            current_pos = end + 1  # account for space\n",
        "\n",
        "        # Build entity spans from BIO tags\n",
        "        ents = []\n",
        "        current_ent = None\n",
        "        for i, tag_id in enumerate(ner_tags):\n",
        "            label = label_list[tag_id]\n",
        "            if label == \"O\":\n",
        "                if current_ent:\n",
        "                    ents.append(current_ent)\n",
        "                    current_ent = None\n",
        "                continue\n",
        "\n",
        "            prefix, ent_label = label.split(\"-\")\n",
        "            start_char, end_char = token_offsets[i]\n",
        "\n",
        "            if prefix == \"B\":\n",
        "                if current_ent:\n",
        "                    ents.append(current_ent)\n",
        "                current_ent = (start_char, end_char, ent_label)\n",
        "            elif prefix == \"I\" and current_ent and current_ent[2] == ent_label:\n",
        "                current_ent = (current_ent[0], end_char, ent_label)\n",
        "            else:\n",
        "                if current_ent:\n",
        "                    ents.append(current_ent)\n",
        "                current_ent = None\n",
        "\n",
        "        if current_ent:\n",
        "            ents.append(current_ent)\n",
        "\n",
        "        # Create spans and add to Doc\n",
        "        span_ents = []\n",
        "        for start, end, label in ents:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span:\n",
        "                span_ents.append(span)\n",
        "        doc.ents = span_ents\n",
        "        db.add(doc)\n",
        "    db.to_disk(output_path)"
      ],
      "metadata": {
        "id": "vbEoPYpqt_pp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_spacy(dataset[\"train\"], \"train.spacy\", label_list)\n",
        "convert_to_spacy(dataset[\"validation\"], \"dev.spacy\", label_list)"
      ],
      "metadata": {
        "id": "gBzL3DNLuHRf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Zh9MD_shiJ",
        "outputId": "e5362c30-e006-4154-8846-56a340dc196c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.cli.train import train\n",
        "train(\"./config.cfg\", output_path=\"training_output\",overrides={\"paths.train\": \"./train.spacy\", \"paths.dev\": \"./dev.spacy\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeVaz2AusiW2",
        "outputId": "166ae1e5-4920-4965-b034-788cf895f313"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: training_output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     44.28    0.00    0.00    0.00    0.00\n",
            "  0     200        277.27   2996.40   52.89   54.93   50.99    0.53\n",
            "  0     400        290.58   2281.65   68.11   68.10   68.13    0.68\n",
            "  0     600        240.43   1997.41   74.02   75.14   72.94    0.74\n",
            "  0     800        432.80   1988.48   78.25   78.99   77.52    0.78\n",
            "  0    1000       1128.01   2315.70   81.26   82.26   80.29    0.81\n",
            "  1    1200        461.59   2012.91   83.30   83.79   82.82    0.83\n",
            "  1    1400        478.27   1745.72   83.11   83.09   83.12    0.83\n",
            "  1    1600        635.11   2047.83   85.33   85.62   85.04    0.85\n",
            "  2    1800        730.35   2035.82   86.16   86.47   85.85    0.86\n",
            "  2    2000        735.74   1799.27   86.16   87.04   85.31    0.86\n",
            "  3    2200        884.35   1792.32   86.67   87.27   86.08    0.87\n",
            "  4    2400        993.18   1831.85   87.39   88.33   86.47    0.87\n",
            "  5    2600        950.96   1589.32   87.13   87.60   86.67    0.87\n",
            "  6    2800        837.28   1304.24   87.14   87.61   86.67    0.87\n",
            "  7    3000        859.02   1114.47   87.04   87.65   86.44    0.87\n",
            "  8    3200        873.37    973.13   87.29   87.40   87.18    0.87\n",
            "  9    3400        785.70    869.22   87.75   87.92   87.58    0.88\n",
            "  9    3600        851.84    797.20   88.33   88.70   87.97    0.88\n",
            " 10    3800        996.88    744.58   87.82   88.16   87.48    0.88\n",
            " 11    4000        683.57    638.38   87.93   88.52   87.34    0.88\n",
            " 12    4200        748.09    585.52   87.48   87.90   87.06    0.87\n",
            " 13    4400        779.36    567.05   86.94   87.42   86.45    0.87\n",
            " 14    4600        826.23    516.80   87.36   87.47   87.26    0.87\n",
            " 15    4800        743.36    491.17   87.47   87.79   87.14    0.87\n",
            " 16    5000        904.15    509.17   87.88   88.15   87.61    0.88\n",
            " 17    5200        859.43    480.70   87.88   88.19   87.58    0.88\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training_output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"./training_output/model-best\")\n",
        "nlp.to_disk(\"my_ner_model\")"
      ],
      "metadata": {
        "id": "CiXVqobOvqrP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_spacy(dataset[\"test\"], \"test.spacy\", label_list)"
      ],
      "metadata": {
        "id": "bZZpWN1l5iYl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate my_ner_model test.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxpD9meT7Jy2",
        "outputId": "4cfa76c6-f295-4b77-c9e1-ab7e6272f45e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   80.51 \n",
            "NER R   81.11 \n",
            "NER F   80.81 \n",
            "SPEED   22324 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "LOC    84.74   86.87   85.79\n",
            "PER    82.44   83.61   83.02\n",
            "ORG    76.79   75.08   75.92\n",
            "MISC   74.44   75.93   75.18\n",
            "\n"
          ]
        }
      ]
    }
  ]
}